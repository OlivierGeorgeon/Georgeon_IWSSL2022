\documentclass[pmlr]{jmlr}% new name PMLR (Proceedings of Machine Learning Research)

 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
 
 % OG from doc https://en.wikibooks.org/wiki/LaTeX/Tables#Floating_with_table 
 \usepackage{multirow}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

 % Define an unnumbered theorem just for this sample document:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{1}
\jmlryear{2010}
\jmlrworkshop{Workshop Title}

\title[SLAPI]{Simultaneous Localization and Active Phenomenon Inference (SLAPI)}

 % Use \Name{Author Name} to specify the name.

 % Spaces are used to separate forenames from the surname so that
 % the surnames can be picked up for the page header and copyright footer.
 
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % *** Make sure there's no spurious space before \nametag ***

 % Two authors with the same address
  \author{\Name{Olivier L. Georgeon} \Email{ogeorgeon@univ-catholyon.fr}\\
  \addr UR Confluence, Sciences et Humanités (EA 1598) - Lyon Catholic University, France\\
   \Name{Titouan Knockaert} \Email{titouan.knockaert@gmail.com}\\
  \addr Université Claude Bernard Lyon 1, LIRIS CNRS UMR5205, F-69622 Villeurbanne, France
}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}

\editor{Editor's name}
 % \editors{List of editors' names}

\begin{document}

\maketitle

\begin{abstract}
This is the abstract for this article.
\end{abstract}
\begin{keywords}
List of keywords
\end{keywords}

\section{Introduction}
\label{sec:intro}

The problem of getting mobile robots to autonomously learn the position of surrounding objects, recognize them, and keep track of their relative displacements is considered by many to be a key prerequisite of truly autonomous robots. 
Within this framework, the SLAM problem (Simultaneous Localization and Mapping) has been formalized and studied since the 1990s: constructing and updating a map of an unknown environment while simultaneously keeping track of the robot's position within it \citep[e.g.,][]{taketomi_visual_2017}.
SLAM algorithms are tailored to the available resources: odometric sensors, sensors of the environment, computational capacities, as well as the landmarks' properties, quantity, and dynamics, and the usage intended for the robot.

When displacements are imprecise and odometric data is not available, when landmarks are not directly identifiable, and below a certain level of scarcity and noise in the sensory data relative to the environment’s complexity, it becomes difficult to perform SLAM accurately enough to use the robot for tasks involving complex navigation \citep{gay_towards_2021}. 
For such robots, we propose the SLAPI problem:  Simultaneous Localization and Active Phenomenon Inference.
In contrast with SLAM, SLAPI does not aim at constructing a map to use for navigation. 
Instead, it aims at organizing behavior spatially in the vicinity of objects to design robots that exhibit intrinsic motivation \citep[e.g.,][]{oudeyer_intrinsic_2007} such as playfulness and curiosity as they discover and interact with unknown objects. 
Possible applications may not include delivery tasks but may include entertainment and games, similar to playing with pets. 

SLAPI makes no assumption that landmarks can be directly and passively uniquely identified through sensors. 
The robot must rather actively interact with objects, possibly from different angles and through different modalities of control loops, to categorize and recognize objects, and possibly use them as landmarks. 
We call this process \textit{active phenomenon inference}, drawing from the work of \cite{friston_world_2021} on active inference. 
Here the term \textit{phenomenon} refers to the knowledge of physical objects actively constructed by the robot from its point of view and ``as the robot experiences the object through interaction'' \citep{thorisson_explanation_2021}. 

We designed a proof-of-concept algorithm to illustrate the SLAPI problem. 
We demonstrated it in a robot mounted on omnidirectional wheels and endowed with an echo-localization sensor, photosensitive sensors, and an inertial measurement unit, but no camera and lidar. 
As the robot circles around an object, it constructs the phenomenon corresponding to this object under the form of the set of the spatially-localized control loops that the object affords to the robot. 
New elements can be subsequently added to this set as the robot improves its knowledge of the object. 
Results show that the robot drew out a few cries of amusement and endearment from some human observers, which encourages us to keep improving this range of algorithms. 

Moreover, we believe that the study of SLAPI problems can shed some light on how animals construct knowledge of objects through sensorimotor interactions, while keeping this knowledge grounded in experience. 
It can also provide an angle of attack to the more general AI problem of self-motivated open-ended learning in the real world.

\section{The representational status of sensory data}
\label{sec:input}

An autonomous agent faces the necessity to actively infer the existence and the properties of objects in its environment when such existence and properties are not directly registered in sensory data. 
This raises the question of the \textit{representational status of sensory} data: is sensory data representational or not? 
This question has been discussed times and again at the philosophical level \citep[e.g.,][]{williford_husserls_2013}.
Loosely, two hypotheses collide: the hypothesis that sensory data carry information about features of the world, versus the hypothesis that sensory data carry information about the agent's experience of interaction with the world. 
We refer to the former as the \textit{representationalist hypothesis}, and to the latter as the \textit{constructivist hypothesis} because it relates to Piaget's theory of construcitivist learning based on sensorimotor schemes \citep{guillermin_artificial_2022}. 

SLAPI falls within the constructivist hypothesis because it applies to robots that have coarse sensors that do not provide much descriptive information about the environment. 
The robot must probe the environment a little bit like a blind person who uses a cane to actively construct a mental representation of its surroundings. 
Probing experiences consist of control loops during which the robot interacts with the environment.   
They are triggered by an action intended by the robot and result in an outcome that is informative not of the object itself but of the possibility of interaction afforded by the object to the robot. 
\figureref{fig:cycle} shows this cycle of interaction. 
The software selects an action associated with spatial information that specify how the control loop should be enacted in the world. 
In return, the software receives an outcome associated with spatial information that describes how the control loop has been enacted depending on the actual nature and position of surrounding objects. 


\begin{figure}[htbp]
	% Caption and label go in the first argument and the figure contents
	% go in the second argument
	\floatconts
	{fig:cycle}
	{\caption{The interaction cycle. Black bullet: the cycle begins with the software selecting an action containing spatial information to enact in the world (right). Black arrowhead: the cycle ends with the software receiving the outcome containing spatial information (left).}}
	{\includegraphics[width=0.8\linewidth]{images/Figure_0_Cycle}}
\end{figure}

Note that the constructivist hypothesis accepts that the outcome may sometimes register features of the environment but only avoids baking this assumption in the algorithm \textit{a priori}. 
\cite{rudrauf_mathematical_2017} have also proposed an active inference model related to the constructivist hypothesis. He states that ``All we need here is the idea that in one way or another the sensory organs provide an independent source of input and correction for the continually updated world model'' (p. 19).

\section{The experimental setup}
\label{sec:experiment}

We use the robot cat of brand Osoyoo\footnote{\url{https://osoyoo.com/2019/11/08/omni-direction-mecanum-wheel-robotic-kit-v1/}}, to which we added an inertial measurement unit (\figureref{fig:robot}). 
We set up an environment with black lines on the floor and various objects that can be detected through the echo-localization sensor. 
Our goal is to design a robot that explores such environment and constructs knowledge of the objects that it encounters to exhibit behaviors that look interesting to human observers. 

\begin{figure}[htbp]
	% Caption and label go in the first argument and the figure contents
	% go in the second argument
	\floatconts
	{fig:robot}
	{\caption{The experimental setup. Osoyoo robots, a black line on the floor, and various objects detectable through echo-localization.}}
	{\includegraphics[width=0.8\linewidth]{images/Figure_1_Robotb}}
\end{figure}

\section{The software architecture and algorithm}
\label{sec:software}

We implemented the cognitive architecture on a remote PC as depicted in \figureref{fig:architecture}. 
This architecture takes the place of the \textit{Software} in \figureref{fig:cycle}, whereas the robot plus its environment takes the place of the \textit{World}.

The architecture follows a regular Model-View-Controller design pattern in which the \textit{Memory} plays the role of the database, and the \textit{Workspace} plays the role of the Model. 
The \textit{Memory} contains the Egocentric memory and the Allocentric memory which are displayed on screen via the \textit{View Controllers} (\figureref{fig:architecture}, right).
The \textit{Workspace} contains the \textit{Synthesizer} which infers the phenomena, and the \textit{Decider} which selects the next action to send to the robot through wifi. 

\begin{figure}[htbp]
	% Caption and label go in the first argument and the figure contents
	% go in the second argument
	\floatconts
	{fig:architecture}
	{\caption{The software architecture implemented on a remote PC. 
			Bottom: the robot recieves the action and sends back the outcome through wifi.
			Top-right: Egocentric memory shows the position of echos (orange circles), the focus of attention (red hexagon), and the previous positions of the robot (green triangles).
			Bottom-right: Allocentric memory shows the position of the echo (orange cells), the explored cells (white), the position of the robot (green), the unexpored cells (grey).}}
	{\includegraphics[width=0.8\linewidth]{images/Figure_2_Architecture}}
\end{figure}


\subsection{The primitive control loops implemented in the robot}

A C++ program on the robot's Arduino board handles the reception of the action, drives the control loop, and then returns the outcome through wifi. 
\tableref{tab:action} lists the supported actions and outcomes.

The reception of an action triggers the enaction of the corresponding control loop until its termination condition.
For example, the \texttt{Forward} action sets the robot in motion. 
It has three possible termination conditions: 
\texttt{Default}: Time out of 1 second (approximately 20 cm traveled).
\texttt{Line detection}: The floor sensor detects a line; the robot retreats back for a few centimeters. 
\texttt{Shock}: The inertial measurement unit detects a strong deceleration indicating a shock with a solid object.


\begin{table}[htbp]
	\floatconts
	{tab:action}
	{\caption{Actions available to the robot and their possible outcomes}}
	{%
		\subtable[Actions]{%
			\label{tab:ab}%
			\begin{tabular}{lc}
				\toprule
				\bfseries Code & \bfseries Description\\
				\midrule
				\texttt{Forward} & \multirow{4}{2.5cm}{During 1 sec. or until shock or line detection}\\
				\texttt{Backward} &  \\
				\texttt{Shift left} &  \\
				\texttt{Shift right} & \\ \hline
				\texttt{Turn left} & $\pi/4$ \\
				\texttt{Turn right} & $-\pi/4$ \\ \hline
				\texttt{Head scan} & $[-\pi/2, \pi/2]$ \\
				\bottomrule
		\end{tabular}}
		\qquad
		\subtable[Outcomes]{%
			\label{tab:cd}%
			\begin{tabular}{lc}
				\toprule
				\bfseries Code & \bfseries Description\\
				\midrule
				\texttt{Line left} & \multirow{3}{2.8cm}{Floor sensors cross luminosity threshold } \\
				\texttt{Line front} & \\
				\texttt{Line right} & \\ \hline
				\texttt{Shock} & Violent deceleration \\ \hline
				\texttt{Echo lost focus} & No echo where expected \\ \hline
				\texttt{Echo left} & \multirow{6}{3cm}{Direction and range of the nearest echo}\\  
				\texttt{Echo right} &  \\  
				\texttt{Echo far left} & \\  
				\texttt{Echo far right} & \\  
				\texttt{Echo far front} & \\
				\texttt{Echo close front} & \\ \hline
				\texttt{Default} & No line, no echo \\ 
				\bottomrule
			\end{tabular}
		}
	}
\end{table}

Besides the action code, the cognitive architecture sends two more values to the robot: the coordinates of a focus point, and an estimated speed. 
The robot's program uses these to keep the robot's head aligned towards the focus point during the robot's displacement. 
This helps the robot keep tracks of objects over various moves. 
It also gives the human observer the impression that the robot keeps it attention to a particular object, making it look more alive. 

The robot also returns additional information to the cognitive architecture: the yaw, azimuth (angle relative to the north) and duration of the various phases of the control loop. 
The cognitive architecture uses this information to update the spatial memory based on the robot's displacement. 
Moreover, the robot's program controls the head so that it aligns itself towards the nearest echo. 
The robot then returns the nearest echo measure along with the head direction. 
The cognitive architecure uses this information to mark the position of echos in spatial memory. 
This will be used to infer the presence of an object in the area. 
\tableref{tab:dialogue} summarizes the data exchanged between the PC and the robot.

\begin{table}[htbp]
	% The first argument is the label.
	% The caption goes in the second argument, and the table contents
	% go in the third argument.
	\floatconts
	{tab:dialogue}%
	{\caption{Dialogue between the PC and the robot through wifi}}%
	{\begin{tabular}{l|l}
			\toprule
			PC to Robot & Action code, focus coordinates $(x, y)$, estimated speed $(x, y)$\\
			\midrule
			Robot to PC & Outcome code, echo distance, head direction, yaw, azimuth, duration\\
			\bottomrule
	\end{tabular}}
\end{table}

\subsection{The egocentric memory}

The egocentric memory stores the robot's \textit{experiences} in a coordinate system centered on the robot.
It is inspired by the brain's egocentric cells located in the superior colliculus \citep{grieves_representation_2017}.

An experience is a data structure that contains the action, the outcome, the position in space relative to the robot, and the timestamp. 
The position in space corresponds to where the interaction took place, for example under the floor sensors for a line detection, or at the estimated origin of an echo. 
The positions of experiences are updated with the robot's displacement on each interaction cycle. 

Of course, errors accumulate causing a drift in the position of experiences, making them unreliable after a few cycles. 
In the screen display (\figureref{fig:architecture}, top right), we make them fade away and disappear after 5 cycles.  


\subsection{The allocentric memory}

The allocentric memory stores a spatial representation in a coordinate system relative the environment. 
It is inspired by \textit{grid cells} in the hippocampus of the mammalian brain \citep{grieves_representation_2017}.
Like natural grid cells, we implemented an hexagonal cell grid (\figureref{fig:architecture}, bottom right). 
Each cell stores the list of experiences in a particular place, and can have one of the following status:

\begin{altdescription}{Phenomenon}
	\item[Unknown] No experience has been attempted here.
	\item[Occupied] The robot is placed here.
	\item[Empty] The robot has been here in the past.
	\item[Experience] One or several experiences have been placed here.
\end{altdescription}

Allocentric memory is used to construct a local spatial representation of the objects with which the robot is interacting.
This representation is copied to a data structure called a \textit{phenomenon}.

We use the term phenomenon in its common sense: the perception by a cognitive beeing of ``something'' in the environment.
In computer science, we comply with \cite{thorisson_explanation_2021}'s definition of ``Any useful grouping of a subset of spatio-temporal patterns experienced by an agent in an environment''.

In our implementation, the phenomenon data structure stores a local map of experiences afforded by the object. 
This map is a subset of egocentric memory converted to a coordinate system centered on the phenomenon.
This process of phenomenon inference is performed by the Synthesizer.

\subsection{The synthesizer}
\label{sec:synthesizer}


The synthesizer constructs a representation of an object by reading experiences from egocentric memory and filling and updating allocentric memory. 
It works in coordination with the Decider (\sectionref{sec:decider}) to explore the object through different directions. 

The synthesizer estimates the robot's position in allocentric memory through path integration. 
It infers the presence of an object from repeated experiences in the same area. 
Once an object has been identified with enough experiences, it can be isolated in allocentric memory and copied to a phenomenon data structure.


\subsection{The decider}
\label{sec:decider}


\section{Results}

\section{Conclusion}

This example is meant to illustrate a more profound epistemological assumption that the world \textit{in itself} cannot be accessed through direct representational data. This idea goes back to Kant, relates to Piaget's developmental psychology and constructivist epistemology, and has echos in modern physics. 
Kark Friston said that addressing this problem may open the door to the third wave of AI \cite[time code 22:66]{videoFriston}. 



\bibliography{olivier}


\end{document}
